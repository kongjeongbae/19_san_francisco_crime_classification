{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/sf-crime/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/sf-crime/test.csv.zip')\nsub = pd.read_csv('/kaggle/input/sf-crime/sampleSubmission.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df = pd.concat([train, test], sort = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df['year'] = all_df['Dates'].astype('datetime64').dt.year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n# countplot - \n# barplot - \nplt.figure(figsize = (15,10))\nsns.barplot(all_df['Category'], all_df['year'])\nplt.ylim(2005, 2010) # y limit\nplt.xticks(rotation=75) # sns.barplot 밑에 써야함\n\n# 요새 많이 발생하는 범죄, 예전에 발생하던 범죄를 볼 수 있음\n# 검은색 선은 편차. 검은색 선이 길면 편차가 심함. 짧으면 저 연도에만 특히 일어났다고 해석하면 됨.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_df Address에 / 이 들어있냐, Block of 들어있는지 확인하는 피쳐 넣으면 점수 오름\n\nall_df['isBlock'] = all_df['Address'].str.contains('Block', case=False) # case 대소문자 구분 안한다는 옵션(uppercase, lowercase 이런말인듯)\n\nall_df['isAV'] = all_df['Address'].str.contains('AV', case=False) # case 대소문자 구분 안한다는 옵션(uppercase, lowercase 이런말인듯)\nall_df['isSla'] = all_df['Address'].str.contains('/', case=False) # case 대소문자 구분 안한다는 옵션(uppercase, lowercase 이런말인듯)\nall_df['isStreet'] = all_df['Address'].str.contains('st', case=False) # case 대소문자 구분 안한다는 옵션(uppercase, lowercase 이런말인듯)\n\n\n# all_df.groupby('Category')['isBlock'].mean()\n# barplot, countplot 봐보자!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\ntk = Tokenizer()\ntk.fit_on_texts(all_df['Address'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text = tk.texts_to_sequences(all_df['Address'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\npad_text = pad_sequences(all_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pad_text[0] # pad를 앞에 채우는 것이 뒤에 채우는 것보다 점수가 보통 더 높음. \n# 왜냐 앞쪽에 중요한 말이 안나옴. 보통 뒷쪽에 중요한 말이 나옴. '안녕하세요? 잘 지내시죠? 저는 캐글러입니다.' 이처럼 앞은 인사말처럼 똑같은 말 나옴.\n# 캐글러라는 중요한 정보는 뒤에있음.\n# 문장이 길면 pad 잘라서 활용해야 함. 여기선 짧으니 그냥 사용","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pad_train = pad_text[:len(train)]\npad_test = pad_text[len(train):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tk.word_index # 가장 많이 등장한 순서대로 index 찍힘. isOf, isBlock, isMission, isMarket 이런 피쳐 넣어주면 도움.\n# padding 때문에 0 도 있음. Embedding(len(tk.word_index) + 1) // +1 꼭 해주자","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 아웃풋을 39개로 정해서 모델 만들어서 피쳐로 활용해보자.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import Sequential\nfrom keras.layers import Embedding, Dense, Flatten\n\nmodel = Sequential()\nmodel.add(Embedding(len(tk.word_index) + 1, 10, input_length=len(pad_text[0])))\n# padding 때문에 0 도 있음. Embedding(len(tk.word_index) + 1) // +1 꼭 해주자, 2번째인자는 임베딩 차원, 3번째 인자는 단어의 길이\nmodel.add(Flatten())\nmodel.add(Dense(39, activation='softmax')) # 정답값 39개","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()\n# flatten_1 (Flatten)          (None, 9)                 0       9*1 해서 9","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n    metrics=['acc'], # 대괄호 까먹지 말자\n    loss='sparse_categorical_crossentropy', # 머신러닝에서 딥러닝 쓸 때 sparse 씀. 딥러닝은 무조건 숫자여야함. y값을 숫자로 바꿔주기 위해서.\n    optimizer='adam'\n\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny = le.fit_transform(train['Category'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(pad_train, y, batch_size = 512, epochs=5) # 단어를 학습시키는 것!!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result1 = model.predict(pad_train)\nresult2 = model.predict(pad_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result1[0] # 39차원 pca활용해서 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\npca.fit(pd.concat([pd.DataFrame(result1), pd.DataFrame(result2)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_train = pca.transform(result1)\nresult_test = pca.transform(result2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca = pd.concat([pd.DataFrame(result_train), pd.DataFrame(result_test)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df = pd.concat([all_df, df_pca], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df['month'] = all_df['Dates'].astype('Datetime64').dt.month\nall_df['day'] = all_df['Dates'].astype('Datetime64').dt.day\nall_df['hour'] = all_df['Dates'].astype('Datetime64').dt.hour\nall_df['minute'] = all_df['Dates'].astype('Datetime64').dt.minute\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 최초 날짜부터 몇일 지났는지 새로운 피쳐 추가하자.\nall_df['ndays'] = all_df['Dates'].astype('Datetime64').dt.date - all_df['Dates'].astype('Datetime64').dt.date.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df['ndays'] = all_df['ndays'].apply(lambda x: x.days)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df['x_minus_y'] = all_df['X'] - all_df['Y']\nall_df['x_plus_y'] = all_df['X'] + all_df['Y']\n# 45도 회전변환, 새로운 공간의 좌표를 표시해줄수있음","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# minute이 도움이 되는 이유\n# boxplot, groupby으로 보기 힘듬, 둘다 카테고리임 minute이랑 category 피쳐 둘다 카테고리\n# 이때는, countplot이 좋음\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntrain['minute'] = train['Dates'].astype('Datetime64').dt.minute\nplt.figure(figsize=(15,10))\nsns.countplot(train['minute'], hue=train['Category']) # 클래스 39개라 그림이 너무 이상해짐, 많은 범죄 5개로 볼수있음.\nplt.xlim(-0.5, 1.5)\n# 분마다 클래스들의 비율이 달라짐 따라서 도움이 됨.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df = all_df.drop(['Dates', 'Category', 'Descript', 'Id', 'Resolution'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_df.nunique()\n# Address   24777 unique 값이 너무 많음\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nall_df['DayOfWeek'] = le.fit_transform(all_df['DayOfWeek'])\nall_df['PdDistrict'] = le.fit_transform(all_df['PdDistrict'])\nall_df['Address'] = le.fit_transform(all_df['Address']) # 문장 전처리 방법 알아야한다. 위에서 작업했어도남겨두자","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = all_df[:len(train)] \ntest2 = all_df[len(train):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(train2, train['Category'], test_size=0.2, random_state=33, stratify=train['Category'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nlgb = LGBMClassifier(num_leaves= 100, learning_rate=0.025, n_estimators=200)\nlgb.fit(x_train, y_train, eval_set=(x_valid, y_valid))\n\n# 연도 추가했더니 점수 더 나빠짐 - 왜그럴까? - address 가 매우 중요해서 year 추가해도 점수가 안오르거나 오히려 떨어짐.\n# catboost도 사용해서 앙상블쓰자 - 다만 느린게 함정, gpu 쓰면 훨씬 빨라짐.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier\n\ncb = CatBoostClassifier(task_type='GPU')\ncb.fit(x_train, y_train, eval_set=(x_valid, y_valid), early_stopping_rounds=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 파라미터 조절 및 앙상블 하자.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}